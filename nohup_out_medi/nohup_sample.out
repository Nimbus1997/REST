/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
Setting up a new session...
wandb: Currently logged in as: ellen (use `wandb login --relogin` to force relogin)
----------------- Options ---------------
               batch_size: 4                             	[default: 2]
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 512                           	[default: 256]
                 dataroot: /home/guest1/ellen_data/UKB_quality_data2_combined/input_20220623_512_n1000	[default: None]
             dataset_mode: unaligned                     
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: 1                             
            display_ncols: 4                             
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: lsgan                         
                  gpu_ids: 1                             	[default: 0]
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                 lambda_A: 10.0                          
                 lambda_B: 10.0                          
          lambda_identity: 0.5                           
                load_iter: 0                             	[default: 0]
                load_size: 512                           	[default: 286]
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: cycle_gan                     
                 n_epochs: 100                           
           n_epochs_decay: 300                           
               n_layers_D: 3                             
                     name: ellen_dwt_uresnet1_1_512n1000_0831Â 	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: ellen_dwt_uresnet1_1          	[default: resnet_9blocks]
                      ngf: 64                            
               no_dropout: True                          
                  no_flip: True                          	[default: False]
                  no_html: False                         
                     norm: instance                      
              num_threads: 2                             
                output_nc: 3                             
                 patience: 10                            	[default: 5]
                    phase: train                         
                pool_size: 50                            
               preprocess: resize_and_crop               
               print_freq: 100                           
             save_by_iter: False                         
          save_epoch_freq: 5                             
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                use_wandb: True                          	[default: False]
                  verbose: False                         
----------------- End -------------------
dataset [UnalignedDataset] was created
The number of training images = 600
dataset [UnalignedDataset] was created
patience: 10
[UnetSkipConnectionBlock(
  (model): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): UnetSkipConnectionBlock(
      (model): Sequential(
        (0): LeakyReLU(negative_slope=0.2, inplace=True)
        (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (3): UnetSkipConnectionBlock(
          (model): Sequential(
            (0): LeakyReLU(negative_slope=0.2, inplace=True)
            (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            (2): ReLU(inplace=True)
            (3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): ReLU(inplace=True)
        (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (6): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (4): Tanh()
  )
)]
initialize network with normal
[UnetSkipConnectionBlock(
  (model): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): UnetSkipConnectionBlock(
      (model): Sequential(
        (0): LeakyReLU(negative_slope=0.2, inplace=True)
        (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (3): UnetSkipConnectionBlock(
          (model): Sequential(
            (0): LeakyReLU(negative_slope=0.2, inplace=True)
            (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            (2): ReLU(inplace=True)
            (3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): ReLU(inplace=True)
        (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (6): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (4): Tanh()
  )
)]
initialize network with normal
initialize network with normal
initialize network with normal
model [CycleGANModel] was created
---------- Networks initialized -------------
[Network G_A] Total number of parameters : 0.928 M
[Network G_B] Total number of parameters : 0.928 M
[Network D_A] Total number of parameters : 2.765 M
[Network D_B] Total number of parameters : 2.765 M
-----------------------------------------------
wandb: wandb version 0.13.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.9
wandb: Syncing run ellen_dwt_uresnet1_1_512n1000_0831Â 
wandb: â­ï¸ View project at https://wandb.ai/ellen/CycleGAN-and-pix2pix
wandb: ðŸš€ View run at https://wandb.ai/ellen/CycleGAN-and-pix2pix/runs/2fn9nd0i
wandb: Run data is saved locally in /home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/wandb/run-20220831_220329-2fn9nd0i
wandb: Run `wandb offline` to turn off syncing.
/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)

create web directory ./checkpoints/ellen_dwt_uresnet1_1_512n1000_0831Â /web...
learning rate 0.0002000 -> 0.0002000
Traceback (most recent call last):
  File "train.py", line 97, in <module>
    model.optimize_parameters()
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py", line 183, in optimize_parameters
    self.forward()      # compute fake images and reconstruction images.
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py", line 115, in forward
    self.rec_A = self.netG_B(self.fake_B)   # G_B(G_A(A))
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 166, in forward
    return self.module(*inputs[0], **kwargs[0])
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/networks.py", line 957, in forward
    low_result = self.model(low_fq)  # low_result:[1,3,256,256]
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/networks.py", line 607, in forward
    return self.model(x)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/networks.py", line 609, in forward
    return torch.cat([x, self.model(x)], 1)
RuntimeError: Sizes of tensors must match except in dimension 1. Expected size 129 but got size 128 for tensor number 1 in the list.
wandb: Waiting for W&B process to finish, PID 15048... (failed 1). Press ctrl-c to abort syncing.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Synced ellen_dwt_uresnet1_1_512n1000_0831Â : https://wandb.ai/ellen/CycleGAN-and-pix2pix/runs/2fn9nd0i
wandb: Find logs at: ./wandb/run-20220831_220329-2fn9nd0i/logs/debug.log
wandb: 

/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
Setting up a new session...
wandb: Currently logged in as: ellen (use `wandb login --relogin` to force relogin)
----------------- Options ---------------
               batch_size: 4                             	[default: 2]
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 512                           	[default: 256]
                 dataroot: /home/guest1/ellen_data/UKB_quality_data2_combined/input_20220623_512_n1000	[default: None]
             dataset_mode: unaligned                     
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: 1                             
            display_ncols: 4                             
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: lsgan                         
                  gpu_ids: 1                             	[default: 0]
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                 lambda_A: 10.0                          
                 lambda_B: 10.0                          
          lambda_identity: 0.5                           
                load_iter: 0                             	[default: 0]
                load_size: 512                           	[default: 286]
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: cycle_gan                     
                 n_epochs: 100                           
           n_epochs_decay: 300                           
               n_layers_D: 3                             
                     name: ellen_dwt_uresnet1_1_512n1000_0831	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: ellen_dwt_uresnet1_1          	[default: resnet_9blocks]
                      ngf: 64                            
               no_dropout: True                          
                  no_flip: True                          	[default: False]
                  no_html: False                         
                     norm: instance                      
              num_threads: 2                             
                output_nc: 3                             
                 patience: 10                            	[default: 5]
                    phase: train                         
                pool_size: 50                            
               preprocess: resize_and_crop               
               print_freq: 100                           
             save_by_iter: False                         
          save_epoch_freq: 5                             
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                use_wandb: True                          	[default: False]
                  verbose: False                         
----------------- End -------------------
dataset [UnalignedDataset] was created
The number of training images = 600
dataset [UnalignedDataset] was created
patience: 10
>>>>>>>>>>>>>>>>>[1]
[UnetSkipConnectionBlock(
  (model): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): UnetSkipConnectionBlock(
      (model): Sequential(
        (0): LeakyReLU(negative_slope=0.2, inplace=True)
        (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (3): UnetSkipConnectionBlock(
          (model): Sequential(
            (0): LeakyReLU(negative_slope=0.2, inplace=True)
            (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            (2): ReLU(inplace=True)
            (3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): ReLU(inplace=True)
        (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (6): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (4): Tanh()
  )
)]
initialize network with normal
[UnetSkipConnectionBlock(
  (model): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): UnetSkipConnectionBlock(
      (model): Sequential(
        (0): LeakyReLU(negative_slope=0.2, inplace=True)
        (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (3): UnetSkipConnectionBlock(
          (model): Sequential(
            (0): LeakyReLU(negative_slope=0.2, inplace=True)
            (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            (2): ReLU(inplace=True)
            (3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): ReLU(inplace=True)
        (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (6): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (4): Tanh()
  )
)]
initialize network with normal
initialize network with normal
initialize network with normal
model [CycleGANModel] was created
>>>>>>>>>>>>>>>>>[2]
---------- Networks initialized -------------
[Network G_A] Total number of parameters : 0.928 M
[Network G_B] Total number of parameters : 0.928 M
[Network D_A] Total number of parameters : 2.765 M
[Network D_B] Total number of parameters : 2.765 M
-----------------------------------------------
>>>>>>>>>>>>>>>>>[3]
wandb: wandb version 0.13.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.9
wandb: Syncing run ellen_dwt_uresnet1_1_512n1000_0831
wandb: â­ï¸ View project at https://wandb.ai/ellen/CycleGAN-and-pix2pix
wandb: ðŸš€ View run at https://wandb.ai/ellen/CycleGAN-and-pix2pix/runs/1psa4sdv
wandb: Run data is saved locally in /home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/wandb/run-20220901_165020-1psa4sdv
wandb: Run `wandb offline` to turn off syncing.
/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)

create web directory ./checkpoints/ellen_dwt_uresnet1_1_512n1000_0831/web...
>>>>>>>>>>>>>>>>>[4]
>>>>>>>>>>>>>>>>>[5]
learning rate 0.0002000 -> 0.0002000
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
(epoch: 1, iters: 100, time: 0.137, data: 0.226) D_A: 0.343 G_A: 0.708 cycle_A: 12.034 idt_A: 6.894 D_B: 0.268 G_B: 0.645 cycle_B: 6.890 idt_B: 3.011 
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
(epoch: 1, iters: 200, time: 0.168, data: 0.004) D_A: 0.163 G_A: 0.508 cycle_A: 12.807 idt_A: 6.485 D_B: 0.245 G_B: 0.751 cycle_B: 6.482 idt_B: 3.204 
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
(epoch: 1, iters: 300, time: 0.183, data: 0.004) D_A: 0.437 G_A: 1.711 cycle_A: 12.312 idt_A: 6.347 D_B: 0.395 G_B: 1.100 cycle_B: 6.354 idt_B: 3.079 
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[9]
>>>>>>>>>>>>>>>>>[10]
>>>>>>>>>>>>>>>>>[11]
(epoch: 1, iters: 400, time: 0.913, data: 0.004) D_A: 0.183 G_A: 0.742 cycle_A: 12.077 idt_A: 6.636 D_B: 0.364 G_B: 1.285 cycle_B: 6.648 idt_B: 3.017 
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
(epoch: 1, iters: 500, time: 0.100, data: 0.004) D_A: 0.092 G_A: 0.734 cycle_A: 12.035 idt_A: 5.903 D_B: 0.135 G_B: 0.349 cycle_B: 5.937 idt_B: 3.011 
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
(epoch: 1, iters: 600, time: 0.094, data: 0.004) D_A: 0.032 G_A: 0.832 cycle_A: 10.506 idt_A: 6.015 D_B: 0.328 G_B: 0.616 cycle_B: 6.043 idt_B: 2.647 
>>>>>>>>>>>>>>>>>[12]
End of epoch 1 / 400 	 Time Taken: 79 sec
>>>>>>>>>>>>>>>>>[4]
>>>>>>>>>>>>>>>>>[5]
learning rate 0.0002000 -> 0.0002000
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
(epoch: 2, iters: 100, time: 0.112, data: 0.185) D_A: 0.060 G_A: 0.865 cycle_A: 10.560 idt_A: 5.486 D_B: 0.317 G_B: 1.211 cycle_B: 5.530 idt_B: 2.688 
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[9]
>>>>>>>>>>>>>>>>>[10]
>>>>>>>>>>>>>>>>>[11]
(epoch: 2, iters: 200, time: 0.646, data: 0.010) D_A: 0.057 G_A: 0.818 cycle_A: 9.832 idt_A: 5.817 D_B: 0.059 G_B: 0.876 cycle_B: 5.863 idt_B: 2.478 
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[8]
>>>>>>>>>>>>>>>>>[11]
>>>>>>>>>>>>>>>>>[12]
>>>>>>>>>>>>>>>>>[6]
>>>>>>>>>>>>>>>>>[7]/home/guest1/.conda/envs/ellen/lib/python3.7/multiprocessing/semaphore_tracker.py:144: UserWarning: semaphore_tracker: There appear to be 6 leaked semaphores to clean up at shutdown
  len(cache))
