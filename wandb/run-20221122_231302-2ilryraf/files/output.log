create web directory ./checkpoints/ellen_dwt_uresnet1_1_test0/web...
learning rate 0.0002000 -> 0.0002000
/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
(epoch: 1, iters: 100, time: 0.142, data: 0.141) D_A: 0.372 G_A: 0.845 cycle_A: 12.753 idt_A: 6.440 D_B: 0.428 G_B: 0.913 cycle_B: 6.468 idt_B: 3.186
(epoch: 1, iters: 200, time: 0.061, data: 0.002) D_A: 0.167 G_A: 0.926 cycle_A: 13.531 idt_A: 6.163 D_B: 0.173 G_B: 0.612 cycle_B: 6.259 idt_B: 3.368
Traceback (most recent call last):
  File "train.py", line 121, in <module>
    model.optimize_parameters()
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py", line 194, in optimize_parameters
    self.backward_D_A()      # calculate gradients for D_A
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py", line 145, in backward_D_A
    self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py", line 139, in backward_D_basic
    loss_D.backward()
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/autograd/__init__.py", line 156, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt