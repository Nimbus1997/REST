create web directory ./checkpoints/model2_3_eyeq_256_1024/web...
learning rate 0.0002000 -> 0.0002000
/home/user/miniconda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/user/miniconda/lib/python3.8/site-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
(epoch: 801, iters: 400, time: 0.449, data: 0.453) D_A: 0.439 G_A: 0.468 cycle_A: 4.436 idt_A: 1.871 D_B: 0.269 G_B: 0.384 cycle_B: 1.916 idt_B: 1.084
End of epoch 801 / 1000 	 Time Taken: 67 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 802, iters: 192, time: 0.340, data: 0.008) D_A: 0.251 G_A: 0.381 cycle_A: 3.940 idt_A: 1.533 D_B: 0.273 G_B: 0.391 cycle_B: 1.621 idt_B: 0.963
(epoch: 802, iters: 592, time: 0.128, data: 0.010) D_A: 0.487 G_A: 0.594 cycle_A: 3.050 idt_A: 1.274 D_B: 0.265 G_B: 0.331 cycle_B: 1.408 idt_B: 0.733
End of epoch 802 / 1000 	 Time Taken: 53 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 803, iters: 384, time: 0.345, data: 0.006) D_A: 0.239 G_A: 0.405 cycle_A: 4.260 idt_A: 1.397 D_B: 0.357 G_B: 0.417 cycle_B: 1.640 idt_B: 1.024
End of epoch 803 / 1000 	 Time Taken: 51 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 804, iters: 176, time: 0.344, data: 0.006) D_A: 0.267 G_A: 0.365 cycle_A: 2.673 idt_A: 1.309 D_B: 0.241 G_B: 0.325 cycle_B: 1.432 idt_B: 0.622
(epoch: 804, iters: 576, time: 0.123, data: 0.009) D_A: 0.258 G_A: 0.407 cycle_A: 2.971 idt_A: 0.815 D_B: 0.212 G_B: 0.327 cycle_B: 0.930 idt_B: 0.762
Traceback (most recent call last):
  File "train.py", line 105, in <module>
    model.optimize_parameters()
  File "/home/ellen/RetinaImage_model_MW/models/cycle_gan_model.py", line 189, in optimize_parameters
    self.backward_G()             # calculate gradients for G_A and G_B
  File "/home/ellen/RetinaImage_model_MW/models/cycle_gan_model.py", line 160, in backward_G
    self.idt_A = self.netG_A(self.real_B)
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 78, in parallel_apply
    thread.join()
  File "/home/user/miniconda/lib/python3.8/threading.py", line 1011, in join
    self._wait_for_tstate_lock()
  File "/home/user/miniconda/lib/python3.8/threading.py", line 1027, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3861267) is killed by signal: Killed.