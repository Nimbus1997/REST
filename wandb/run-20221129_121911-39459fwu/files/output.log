/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
create web directory ./checkpoints/ellen_dwt_uresnet2_3_1129/web...
learning rate 0.0002000 -> 0.0002000
/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
(epoch: 1, iters: 32, time: 0.744, data: 0.762) D_A: 1.696 G_A: 2.372 cycle_A: 9.047 idt_A: 4.879 D_B: 3.560 G_B: 5.088 cycle_B: 4.948 idt_B: 2.296
(epoch: 1, iters: 64, time: 0.081, data: 0.762) D_A: 5.685 G_A: 2.576 cycle_A: 7.874 idt_A: 4.850 D_B: 6.743 G_B: 5.263 cycle_B: 4.915 idt_B: 2.039
(epoch: 1, iters: 96, time: 0.067, data: 0.762) D_A: 3.377 G_A: 2.449 cycle_A: 9.170 idt_A: 4.669 D_B: 2.719 G_B: 3.122 cycle_B: 4.635 idt_B: 2.379
(epoch: 1, iters: 128, time: 0.071, data: 0.762) D_A: 3.846 G_A: 2.838 cycle_A: 8.357 idt_A: 4.387 D_B: 1.444 G_B: 1.579 cycle_B: 4.482 idt_B: 2.169
(epoch: 1, iters: 160, time: 0.067, data: 0.762) D_A: 1.833 G_A: 2.814 cycle_A: 8.823 idt_A: 3.946 D_B: 1.105 G_B: 1.725 cycle_B: 3.933 idt_B: 2.284
(epoch: 1, iters: 192, time: 0.067, data: 0.762) D_A: 1.701 G_A: 3.489 cycle_A: 8.146 idt_A: 3.993 D_B: 1.363 G_B: 2.713 cycle_B: 3.839 idt_B: 2.124
(epoch: 1, iters: 224, time: 0.056, data: 0.762) D_A: 0.820 G_A: 1.297 cycle_A: 8.457 idt_A: 3.759 D_B: 1.336 G_B: 2.669 cycle_B: 3.689 idt_B: 2.185
(epoch: 1, iters: 256, time: 0.054, data: 0.762) D_A: 0.635 G_A: 0.817 cycle_A: 7.767 idt_A: 3.499 D_B: 1.177 G_B: 1.744 cycle_B: 3.393 idt_B: 2.032
(epoch: 1, iters: 288, time: 0.057, data: 0.762) D_A: 0.516 G_A: 0.599 cycle_A: 5.831 idt_A: 3.294 D_B: 0.760 G_B: 1.146 cycle_B: 3.150 idt_B: 1.513
(epoch: 1, iters: 320, time: 0.056, data: 0.762) D_A: 0.453 G_A: 0.513 cycle_A: 6.880 idt_A: 3.112 D_B: 0.452 G_B: 0.763 cycle_B: 3.082 idt_B: 1.771
(epoch: 1, iters: 352, time: 0.058, data: 0.762) D_A: 0.439 G_A: 0.487 cycle_A: 6.494 idt_A: 2.797 D_B: 0.491 G_B: 0.664 cycle_B: 2.786 idt_B: 1.683
(epoch: 1, iters: 384, time: 0.058, data: 0.762) D_A: 0.418 G_A: 0.443 cycle_A: 5.549 idt_A: 2.626 D_B: 0.480 G_B: 0.709 cycle_B: 2.605 idt_B: 1.430
(epoch: 1, iters: 416, time: 0.057, data: 0.762) D_A: 0.403 G_A: 0.524 cycle_A: 6.370 idt_A: 2.685 D_B: 0.427 G_B: 0.673 cycle_B: 2.682 idt_B: 1.643
