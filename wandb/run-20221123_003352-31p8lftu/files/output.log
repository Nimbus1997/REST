create web directory ./checkpoints/ellen_dwt_uresnet1_1_test0/web...
learning rate 0.0002000 -> 0.0002000
/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
(epoch: 1, iters: 2, time: 2.896, data: 0.129) D_A: 1.262 G_A: 1.460 cycle_A: 13.641 idt_A: 6.353 D_B: 2.918 G_B: 4.051 cycle_B: 6.353 idt_B: 3.412
(epoch: 1, iters: 4, time: 0.162, data: 0.129) D_A: 7.034 G_A: 5.016 cycle_A: 10.431 idt_A: 6.138 D_B: 15.849 G_B: 17.151 cycle_B: 6.139 idt_B: 2.608
(epoch: 1, iters: 6, time: 0.100, data: 0.129) D_A: 4.019 G_A: 3.843 cycle_A: 12.208 idt_A: 6.519 D_B: 2.050 G_B: 3.552 cycle_B: 6.517 idt_B: 3.053
(epoch: 1, iters: 8, time: 0.107, data: 0.129) D_A: 2.020 G_A: 2.074 cycle_A: 12.114 idt_A: 6.653 D_B: 2.366 G_B: 2.336 cycle_B: 6.654 idt_B: 3.030
(epoch: 1, iters: 10, time: 0.116, data: 0.129) D_A: 1.171 G_A: 2.068 cycle_A: 12.695 idt_A: 6.564 D_B: 1.097 G_B: 1.405 cycle_B: 6.566 idt_B: 3.176
(epoch: 1, iters: 12, time: 0.136, data: 0.129) D_A: 0.857 G_A: 1.258 cycle_A: 11.359 idt_A: 6.606 D_B: 0.656 G_B: 0.875 cycle_B: 6.609 idt_B: 2.841
(epoch: 1, iters: 14, time: 0.143, data: 0.129) D_A: 0.772 G_A: 1.310 cycle_A: 13.283 idt_A: 6.687 D_B: 0.660 G_B: 0.811 cycle_B: 6.691 idt_B: 3.322
(epoch: 1, iters: 16, time: 0.107, data: 0.129) D_A: 0.791 G_A: 1.574 cycle_A: 12.767 idt_A: 6.481 D_B: 0.542 G_B: 0.816 cycle_B: 6.486 idt_B: 3.193
(epoch: 1, iters: 18, time: 0.115, data: 0.129) D_A: 0.799 G_A: 1.594 cycle_A: 12.528 idt_A: 6.465 D_B: 0.384 G_B: 0.713 cycle_B: 6.468 idt_B: 3.133
(epoch: 1, iters: 20, time: 0.101, data: 0.129) D_A: 0.400 G_A: 0.983 cycle_A: 14.226 idt_A: 6.784 D_B: 0.358 G_B: 0.695 cycle_B: 6.789 idt_B: 3.557
(epoch: 1, iters: 22, time: 0.111, data: 0.129) D_A: 0.414 G_A: 1.049 cycle_A: 13.063 idt_A: 6.127 D_B: 0.276 G_B: 0.814 cycle_B: 6.129 idt_B: 3.267
(epoch: 1, iters: 24, time: 0.105, data: 0.129) D_A: 0.469 G_A: 1.092 cycle_A: 12.781 idt_A: 6.780 D_B: 0.303 G_B: 0.785 cycle_B: 6.782 idt_B: 3.196
(epoch: 1, iters: 26, time: 0.160, data: 0.129) D_A: 0.623 G_A: 1.492 cycle_A: 13.501 idt_A: 6.096 D_B: 0.336 G_B: 0.782 cycle_B: 6.098 idt_B: 3.376
(epoch: 1, iters: 28, time: 0.132, data: 0.129) D_A: 0.673 G_A: 1.658 cycle_A: 12.319 idt_A: 6.711 D_B: 0.362 G_B: 0.701 cycle_B: 6.716 idt_B: 3.079
(epoch: 1, iters: 30, time: 0.107, data: 0.129) D_A: 0.448 G_A: 1.200 cycle_A: 14.973 idt_A: 7.126 D_B: 0.259 G_B: 0.716 cycle_B: 7.131 idt_B: 3.743
(epoch: 1, iters: 32, time: 0.104, data: 0.129) D_A: 0.379 G_A: 1.326 cycle_A: 12.727 idt_A: 6.124 D_B: 0.358 G_B: 0.725 cycle_B: 6.127 idt_B: 3.182
(epoch: 1, iters: 34, time: 0.099, data: 0.129) D_A: 0.248 G_A: 0.980 cycle_A: 12.211 idt_A: 5.957 D_B: 0.400 G_B: 0.864 cycle_B: 5.958 idt_B: 3.052
(epoch: 1, iters: 36, time: 0.097, data: 0.129) D_A: 0.301 G_A: 1.143 cycle_A: 12.316 idt_A: 6.034 D_B: 0.620 G_B: 1.089 cycle_B: 6.035 idt_B: 3.075
(epoch: 1, iters: 38, time: 0.096, data: 0.129) D_A: 0.419 G_A: 1.096 cycle_A: 14.685 idt_A: 5.978 D_B: 0.753 G_B: 1.581 cycle_B: 5.979 idt_B: 3.671
(epoch: 1, iters: 40, time: 0.116, data: 0.129) D_A: 0.319 G_A: 1.074 cycle_A: 13.118 idt_A: 6.452 D_B: 0.701 G_B: 1.712 cycle_B: 6.457 idt_B: 3.278
(epoch: 1, iters: 42, time: 0.143, data: 0.129) D_A: 0.212 G_A: 0.988 cycle_A: 13.800 idt_A: 6.602 D_B: 0.345 G_B: 1.062 cycle_B: 6.608 idt_B: 3.449
(epoch: 1, iters: 44, time: 0.126, data: 0.129) D_A: 0.213 G_A: 0.975 cycle_A: 13.182 idt_A: 6.508 D_B: 0.251 G_B: 0.976 cycle_B: 6.519 idt_B: 3.294
(epoch: 1, iters: 46, time: 0.102, data: 0.129) D_A: 0.185 G_A: 0.870 cycle_A: 12.381 idt_A: 6.367 D_B: 0.366 G_B: 0.913 cycle_B: 6.374 idt_B: 3.093
(epoch: 1, iters: 48, time: 0.101, data: 0.129) D_A: 0.208 G_A: 0.870 cycle_A: 13.955 idt_A: 6.275 D_B: 0.435 G_B: 0.883 cycle_B: 6.282 idt_B: 3.489
(epoch: 1, iters: 50, time: 0.112, data: 0.129) D_A: 0.256 G_A: 1.063 cycle_A: 13.288 idt_A: 7.028 D_B: 0.397 G_B: 0.948 cycle_B: 7.042 idt_B: 3.320
(epoch: 1, iters: 52, time: 0.089, data: 0.129) D_A: 0.466 G_A: 0.991 cycle_A: 12.862 idt_A: 5.827 D_B: 0.284 G_B: 0.886 cycle_B: 5.836 idt_B: 3.213
(epoch: 1, iters: 54, time: 0.096, data: 0.129) D_A: 0.249 G_A: 0.682 cycle_A: 13.231 idt_A: 6.711 D_B: 0.235 G_B: 0.661 cycle_B: 6.725 idt_B: 3.305
(epoch: 1, iters: 56, time: 0.147, data: 0.129) D_A: 0.326 G_A: 0.423 cycle_A: 13.148 idt_A: 5.943 D_B: 0.269 G_B: 0.866 cycle_B: 5.948 idt_B: 3.286
(epoch: 1, iters: 58, time: 0.115, data: 0.129) D_A: 0.274 G_A: 0.710 cycle_A: 15.052 idt_A: 6.250 D_B: 0.296 G_B: 0.739 cycle_B: 6.263 idt_B: 3.763
(epoch: 1, iters: 60, time: 0.102, data: 0.129) D_A: 0.267 G_A: 0.981 cycle_A: 14.032 idt_A: 6.330 D_B: 0.309 G_B: 0.875 cycle_B: 6.340 idt_B: 3.508
(epoch: 1, iters: 62, time: 0.096, data: 0.129) D_A: 0.333 G_A: 0.971 cycle_A: 12.142 idt_A: 6.641 D_B: 0.290 G_B: 0.877 cycle_B: 6.659 idt_B: 3.034
(epoch: 1, iters: 64, time: 0.098, data: 0.129) D_A: 0.288 G_A: 0.643 cycle_A: 12.505 idt_A: 6.144 D_B: 0.278 G_B: 0.733 cycle_B: 6.162 idt_B: 3.125
(epoch: 1, iters: 66, time: 0.083, data: 0.129) D_A: 0.279 G_A: 0.680 cycle_A: 12.399 idt_A: 7.017 D_B: 0.341 G_B: 0.712 cycle_B: 7.036 idt_B: 3.099
(epoch: 1, iters: 68, time: 0.096, data: 0.129) D_A: 0.221 G_A: 0.431 cycle_A: 13.272 idt_A: 6.464 D_B: 0.327 G_B: 0.460 cycle_B: 6.486 idt_B: 3.317
Traceback (most recent call last):
  File "train.py", line 126, in <module>
    model.optimize_parameters()
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py", line 192, in optimize_parameters
    self.backward_G()             # calculate gradients for G_A and G_B
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py", line 161, in backward_G
    self.idt_A = self.netG_A(self.real_B)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 172, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/parallel/replicate.py", line 91, in replicate
    param_copies = _broadcast_coalesced_reshape(params, devices, detach)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/parallel/replicate.py", line 71, in _broadcast_coalesced_reshape
    tensor_copies = Broadcast.apply(devices, *tensors)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/parallel/_functions.py", line 14, in forward
    assert all(i.device.type != 'cpu' for i in inputs), (
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/parallel/_functions.py", line 14, in <genexpr>
    assert all(i.device.type != 'cpu' for i in inputs), (
KeyboardInterrupt
(epoch: 1, iters: 70, time: 0.094, data: 0.129) D_A: 0.303 G_A: 0.468 cycle_A: 15.780 idt_A: 6.183 D_B: 0.435 G_B: 0.842 cycle_B: 6.202 idt_B: 3.945
(epoch: 1, iters: 72, time: 0.148, data: 0.129) D_A: 0.308 G_A: 1.093 cycle_A: 13.842 idt_A: 6.437 D_B: 0.744 G_B: 1.085 cycle_B: 6.453 idt_B: 3.460
(epoch: 1, iters: 74, time: 0.120, data: 0.129) D_A: 0.454 G_A: 1.667 cycle_A: 13.094 idt_A: 6.699 D_B: 0.616 G_B: 0.901 cycle_B: 6.721 idt_B: 3.273
(epoch: 1, iters: 76, time: 0.115, data: 0.129) D_A: 0.211 G_A: 0.848 cycle_A: 14.549 idt_A: 6.361 D_B: 0.276 G_B: 0.616 cycle_B: 6.376 idt_B: 3.636
(epoch: 1, iters: 78, time: 0.092, data: 0.129) D_A: 0.275 G_A: 0.447 cycle_A: 15.236 idt_A: 6.138 D_B: 0.389 G_B: 0.458 cycle_B: 6.158 idt_B: 3.808
(epoch: 1, iters: 80, time: 0.104, data: 0.129) D_A: 0.247 G_A: 0.492 cycle_A: 12.659 idt_A: 5.641 D_B: 0.246 G_B: 0.807 cycle_B: 5.659 idt_B: 3.164
(epoch: 1, iters: 82, time: 0.092, data: 0.129) D_A: 0.343 G_A: 1.095 cycle_A: 12.552 idt_A: 6.276 D_B: 0.261 G_B: 0.815 cycle_B: 6.299 idt_B: 3.136