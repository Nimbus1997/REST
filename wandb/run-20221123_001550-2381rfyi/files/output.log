create web directory ./checkpoints/ellen_dwt_uresnet1_1_test0/web...
learning rate 0.0002000 -> 0.0002000
/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
> /home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py(197)optimize_parameters()
-> self.set_requires_grad([self.netD_A, self.netD_B], True)



[?2004h(Pdb) self.loss_G_A
tensor(1.4601, device='cuda:1', grad_fn=<MseLossBackward0>)


[?2004h(Pdb) self.loss_B_A
*** AttributeError: 'CycleGANModel' object has no attribute 'loss_B_A'

[?2004h(Pdb) self.loss_G_B
tensor(4.0510, device='cuda:1', grad_fn=<MseLossBackward0>)


(Pdb) self.set_requires_grad([self.netD_A, self.netD_B], True)

[?2004self.optimizer_D.zero_grad()   # set D_A and D_B's gradients to zero[7mto zero

[?2004self.backward_D_A()      # calculate gradients for D_A[7mfor D_A
[?2004self.loss_D_A[7moss_D_A
tensor(1.2618, device='cuda:1', grad_fn=<MulBackward0>)
[?2004self.backward_D_B()      # calculate graidents for D_B[7mfor D_B

[?2004h(Pdbloss_D_B
tensor(2.9182, device='cuda:1', grad_fn=<MulBackward0>)

[?2004self.optimizer_D.step()  # update D_A and D_B's weights[7mweights














[?2004h(Pdb) self.net_D_A.module.get_parameter('model.0.weight')[0]
*** AttributeError: 'CycleGANModel' object has no attribute 'net_D_A'

[?2004h(Pdb) self.netD_A.module.get_parameter('model.0.weight')[0]]
tensor([[[-0.0060,  0.0092, -0.0209,  0.0098],
         [ 0.0248, -0.0311,  0.0006,  0.0047],
         [ 0.0095, -0.0045, -0.0045,  0.0191],
         [-0.0093, -0.0063,  0.0072, -0.0008]],
        [[ 0.0208,  0.0069, -0.0306,  0.0200],
         [ 0.0281, -0.0057,  0.0018,  0.0336],
         [-0.0064, -0.0225,  0.0189, -0.0101],
         [-0.0240,  0.0023, -0.0309,  0.0059]],
        [[-0.0099, -0.0266, -0.0120,  0.0336],
         [ 0.0303,  0.0291, -0.0131,  0.0030],
         [-0.0549,  0.0125,  0.0122, -0.0148],
         [ 0.0332, -0.0185, -0.0171, -0.0290]]], device='cuda:1',
       grad_fn=<SelectBackward0>)
[?2004h(Pdb)
Traceback (most recent call last):
  File "train.py", line 126, in <module>
    model.optimize_parameters()
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py", line 197, in optimize_parameters
    self.optimizer_D.zero_grad()   # set D_A and D_B's gradients to zero
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py", line 197, in optimize_parameters
    self.optimizer_D.zero_grad()   # set D_A and D_B's gradients to zero
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/bdb.py", line 88, in trace_dispatch
    return self.dispatch_line(frame)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/bdb.py", line 113, in dispatch_line
    if self.quitting: raise BdbQuit
--KeyboardInterrupt--
[?2004h(Pdb) [?2004l