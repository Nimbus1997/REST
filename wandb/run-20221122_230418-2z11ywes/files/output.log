create web directory ./checkpoints/ellen_scatteing_test0/web...
learning rate 0.0002000 -> 0.0002000
/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode)
(epoch: 1, iters: 100, time: 0.332, data: 0.123) D_A: 0.539 G_A: 0.975 cycle_A: 11.759 idt_A: 6.052 D_B: 0.310 G_B: 0.726 cycle_B: 6.115 idt_B: 2.945
(epoch: 1, iters: 200, time: 0.300, data: 0.001) D_A: 0.063 G_A: 0.874 cycle_A: 11.333 idt_A: 5.623 D_B: 0.109 G_B: 0.591 cycle_B: 5.622 idt_B: 2.925
(epoch: 1, iters: 300, time: 0.284, data: 0.002) D_A: 0.050 G_A: 0.877 cycle_A: 7.646 idt_A: 4.713 D_B: 0.104 G_B: 1.050 cycle_B: 4.681 idt_B: 1.903
(epoch: 1, iters: 400, time: 1.114, data: 0.002) D_A: 0.304 G_A: 1.036 cycle_A: 6.512 idt_A: 3.534 D_B: 0.068 G_B: 0.557 cycle_B: 3.668 idt_B: 1.583
(epoch: 1, iters: 500, time: 0.315, data: 0.002) D_A: 0.030 G_A: 0.936 cycle_A: 6.374 idt_A: 3.406 D_B: 0.185 G_B: 0.716 cycle_B: 3.673 idt_B: 1.624
(epoch: 1, iters: 600, time: 0.353, data: 0.002) D_A: 0.024 G_A: 0.762 cycle_A: 4.995 idt_A: 2.850 D_B: 0.029 G_B: 0.770 cycle_B: 2.818 idt_B: 1.227
End of epoch 1 / 400 	 Time Taken: 194 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 2, iters: 100, time: 0.277, data: 0.127) D_A: 0.068 G_A: 0.957 cycle_A: 3.792 idt_A: 2.559 D_B: 0.016 G_B: 1.104 cycle_B: 2.810 idt_B: 0.924
(epoch: 2, iters: 200, time: 0.993, data: 0.002) D_A: 0.048 G_A: 1.104 cycle_A: 3.920 idt_A: 2.092 D_B: 0.027 G_B: 1.034 cycle_B: 2.083 idt_B: 1.141
(epoch: 2, iters: 300, time: 0.268, data: 0.001) D_A: 0.023 G_A: 0.994 cycle_A: 3.001 idt_A: 1.911 D_B: 0.045 G_B: 1.030 cycle_B: 1.997 idt_B: 0.850
(epoch: 2, iters: 400, time: 0.318, data: 0.001) D_A: 0.018 G_A: 0.986 cycle_A: 2.994 idt_A: 1.519 D_B: 0.028 G_B: 0.723 cycle_B: 1.627 idt_B: 1.027
(epoch: 2, iters: 500, time: 0.279, data: 0.002) D_A: 0.027 G_A: 0.807 cycle_A: 2.871 idt_A: 1.815 D_B: 0.057 G_B: 0.521 cycle_B: 2.108 idt_B: 0.726
(epoch: 2, iters: 600, time: 0.631, data: 0.003) D_A: 0.032 G_A: 0.991 cycle_A: 3.152 idt_A: 1.958 D_B: 0.027 G_B: 0.806 cycle_B: 1.780 idt_B: 0.724
End of epoch 2 / 400 	 Time Taken: 190 sec
learning rate 0.0002000 -> 0.0002000
Traceback (most recent call last):
  File "train.py", line 121, in <module>
    model.optimize_parameters()
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py", line 189, in optimize_parameters
    self.backward_G()             # calculate gradients for G_A and G_B
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py", line 179, in backward_G
    self.loss_G.backward()
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/autograd/__init__.py", line 156, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt