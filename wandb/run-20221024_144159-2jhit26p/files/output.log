create web directory ./checkpoints/model2_4_eyeq_256_basicblock_1024/web...
learning rate 0.0002000 -> 0.0002000
/home/user/miniconda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/user/miniconda/lib/python3.8/site-packages/torch/nn/functional.py:3454: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  warnings.warn(
Traceback (most recent call last):
  File "train.py", line 105, in <module>
    model.optimize_parameters()
  File "/home/ellen/RetinaImage_model_MW/models/cycle_gan_model.py", line 189, in optimize_parameters
    self.backward_G()             # calculate gradients for G_A and G_B
  File "/home/ellen/RetinaImage_model_MW/models/cycle_gan_model.py", line 163, in backward_G
    self.idt_B = self.netG_B(self.real_A)
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ellen/RetinaImage_model_MW/models/networks.py", line 1360, in forward
    result_scattering = self.scattering_model(gray_input)
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ellen/RetinaImage_model_MW/models/networks.py", line 1260, in forward
    scattering3 = self.scattering_down_3(out2)
  File "/home/user/miniconda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ellen/RetinaImage_model_MW/models/networks.py", line 1078, in forward
    scatter_ouput = self.Scattering.scattering(x)
  File "/home/user/miniconda/lib/python3.8/site-packages/kymatio/scattering2d/frontend/torch_frontend.py", line 98, in scattering
    S = scattering2d(input, self.pad, self.unpad, self.backend, self.J,
  File "/home/user/miniconda/lib/python3.8/site-packages/kymatio/scattering2d/core/scattering2d.py", line 39, in scattering2d
    U_1_c = rfft(U_1_c)
  File "/home/user/miniconda/lib/python3.8/site-packages/kymatio/scattering2d/backend/torch_backend.py", line 141, in rfft
    return _fft(x_r)
  File "/home/user/miniconda/lib/python3.8/site-packages/kymatio/scattering2d/backend/torch_backend.py", line 10, in <lambda>
    _fft = lambda x: torch.view_as_real(torch.fft.fft2(torch.view_as_complex(x)))
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 7.61 GiB already allocated; 16.69 MiB free; 7.73 GiB reserved in total by PyTorch)