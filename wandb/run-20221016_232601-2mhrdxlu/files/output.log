create web directory ./checkpoints/ellen_dwt_uresnet2_4_512n1000_1016/web...
learning rate 0.0002000 -> 0.0002000
/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.
  "See the documentation of nn.Upsample for details.".format(mode)
(epoch: 1, iters: 100, time: 0.232, data: 0.151) D_A: 0.202 G_A: 0.586 cycle_A: 8.442 idt_A: 5.760 D_B: 0.364 G_B: 0.526 cycle_B: 5.833 idt_B: 2.162
(epoch: 1, iters: 200, time: 0.236, data: 0.002) D_A: 0.297 G_A: 0.252 cycle_A: 3.422 idt_A: 1.696 D_B: 0.297 G_B: 0.212 cycle_B: 1.763 idt_B: 0.841
(epoch: 1, iters: 300, time: 0.234, data: 0.003) D_A: 0.227 G_A: 0.452 cycle_A: 3.508 idt_A: 1.205 D_B: 0.193 G_B: 0.358 cycle_B: 1.234 idt_B: 0.855
(epoch: 1, iters: 400, time: 0.653, data: 0.003) D_A: 0.192 G_A: 0.337 cycle_A: 4.220 idt_A: 1.218 D_B: 0.172 G_B: 0.523 cycle_B: 1.316 idt_B: 1.079
(epoch: 1, iters: 500, time: 0.233, data: 0.002) D_A: 0.193 G_A: 0.375 cycle_A: 4.036 idt_A: 1.147 D_B: 0.131 G_B: 0.463 cycle_B: 1.088 idt_B: 1.035
(epoch: 1, iters: 600, time: 0.248, data: 0.003) D_A: 0.211 G_A: 0.545 cycle_A: 2.820 idt_A: 1.410 D_B: 0.130 G_B: 0.414 cycle_B: 1.231 idt_B: 0.721
End of epoch 1 / 400 	 Time Taken: 152 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 2, iters: 100, time: 0.235, data: 0.209) D_A: 0.178 G_A: 0.634 cycle_A: 2.222 idt_A: 1.776 D_B: 0.201 G_B: 0.516 cycle_B: 1.384 idt_B: 0.549
(epoch: 2, iters: 200, time: 0.601, data: 0.003) D_A: 0.090 G_A: 0.827 cycle_A: 2.381 idt_A: 1.007 D_B: 0.159 G_B: 0.798 cycle_B: 1.304 idt_B: 0.617
(epoch: 2, iters: 300, time: 0.239, data: 0.002) D_A: 0.314 G_A: 0.664 cycle_A: 2.461 idt_A: 2.001 D_B: 0.107 G_B: 0.421 cycle_B: 1.685 idt_B: 0.543
(epoch: 2, iters: 400, time: 0.233, data: 0.003) D_A: 0.185 G_A: 0.662 cycle_A: 2.062 idt_A: 1.503 D_B: 0.163 G_B: 0.519 cycle_B: 1.167 idt_B: 0.508
(epoch: 2, iters: 500, time: 0.232, data: 0.003) D_A: 0.085 G_A: 0.769 cycle_A: 1.664 idt_A: 1.215 D_B: 0.109 G_B: 0.503 cycle_B: 1.104 idt_B: 0.451
(epoch: 2, iters: 600, time: 0.346, data: 0.003) D_A: 0.079 G_A: 0.673 cycle_A: 2.662 idt_A: 0.823 D_B: 0.132 G_B: 0.773 cycle_B: 0.910 idt_B: 0.520
End of epoch 2 / 400 	 Time Taken: 146 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 3, iters: 100, time: 0.246, data: 0.206) D_A: 0.148 G_A: 1.051 cycle_A: 1.801 idt_A: 0.917 D_B: 0.138 G_B: 0.599 cycle_B: 0.915 idt_B: 0.389
(epoch: 3, iters: 200, time: 0.228, data: 0.003) D_A: 0.130 G_A: 0.765 cycle_A: 4.031 idt_A: 0.946 D_B: 0.106 G_B: 0.367 cycle_B: 1.392 idt_B: 0.870
(epoch: 3, iters: 300, time: 0.238, data: 0.003) D_A: 0.079 G_A: 0.816 cycle_A: 1.987 idt_A: 0.883 D_B: 0.146 G_B: 0.518 cycle_B: 1.082 idt_B: 0.460
(epoch: 3, iters: 400, time: 0.574, data: 0.003) D_A: 0.104 G_A: 0.599 cycle_A: 2.802 idt_A: 1.126 D_B: 0.079 G_B: 0.762 cycle_B: 1.234 idt_B: 0.744
(epoch: 3, iters: 500, time: 0.258, data: 0.002) D_A: 0.124 G_A: 0.953 cycle_A: 2.016 idt_A: 1.309 D_B: 0.211 G_B: 0.279 cycle_B: 1.038 idt_B: 0.750
(epoch: 3, iters: 600, time: 0.237, data: 0.002) D_A: 0.139 G_A: 0.709 cycle_A: 1.868 idt_A: 0.935 D_B: 0.213 G_B: 0.401 cycle_B: 1.204 idt_B: 0.432
End of epoch 3 / 400 	 Time Taken: 145 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 4, iters: 100, time: 0.239, data: 0.215) D_A: 0.131 G_A: 0.525 cycle_A: 1.940 idt_A: 0.857 D_B: 0.198 G_B: 0.243 cycle_B: 0.975 idt_B: 0.456
(epoch: 4, iters: 200, time: 0.578, data: 0.002) D_A: 0.134 G_A: 0.400 cycle_A: 1.735 idt_A: 1.128 D_B: 0.173 G_B: 0.406 cycle_B: 1.555 idt_B: 0.461
(epoch: 4, iters: 300, time: 0.237, data: 0.002) D_A: 0.106 G_A: 0.829 cycle_A: 2.209 idt_A: 0.863 D_B: 0.196 G_B: 0.437 cycle_B: 0.986 idt_B: 0.604
Traceback (most recent call last):
  File "train.py", line 105, in <module>
    model.optimize_parameters()
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py", line 189, in optimize_parameters
    self.backward_G()             # calculate gradients for G_A and G_B
  File "/home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/models/cycle_gan_model.py", line 179, in backward_G
    self.loss_G.backward()
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/_tensor.py", line 307, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/autograd/__init__.py", line 156, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt