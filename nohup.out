/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torchvision/transforms/transforms.py:288: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  "Argument interpolation should be of type InterpolationMode instead of int. "
Setting up a new session...
wandb: Currently logged in as: ellen (use `wandb login --relogin` to force relogin)
----------------- Options ---------------
               batch_size: 4                             	[default: 2]
                    beta1: 0.5                           
          checkpoints_dir: ./checkpoints                 
           continue_train: False                         
                crop_size: 512                           	[default: 256]
                 dataroot: /home/guest1/ellen_data/UKB_quality_data2_combined/input_20220623_512_n1000	[default: None]
             dataset_mode: unaligned                     
                direction: AtoB                          
              display_env: main                          
             display_freq: 400                           
               display_id: 1                             
            display_ncols: 4                             
             display_port: 8097                          
           display_server: http://localhost              
          display_winsize: 256                           
                    epoch: latest                        
              epoch_count: 1                             
                 gan_mode: lsgan                         
                  gpu_ids: 1                             	[default: 0]
                init_gain: 0.02                          
                init_type: normal                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                 lambda_A: 10.0                          
                 lambda_B: 10.0                          
          lambda_identity: 0.5                           
                load_iter: 0                             	[default: 0]
                load_size: 512                           	[default: 286]
                       lr: 0.0002                        
           lr_decay_iters: 50                            
                lr_policy: linear                        
         max_dataset_size: inf                           
                    model: cycle_gan                     
                 n_epochs: 100                           
           n_epochs_decay: 300                           
               n_layers_D: 3                             
                     name: ellen_dwt_uresnet1_1_512n1000_0831	[default: experiment_name]
                      ndf: 64                            
                     netD: basic                         
                     netG: ellen_dwt_uresnet1_1          	[default: resnet_9blocks]
                      ngf: 64                            
               no_dropout: True                          
                  no_flip: True                          	[default: False]
                  no_html: False                         
                     norm: instance                      
              num_threads: 2                             
                output_nc: 3                             
                 patience: 10                            	[default: 5]
                    phase: train                         
                pool_size: 50                            
               preprocess: resize_and_crop               
               print_freq: 100                           
             save_by_iter: False                         
          save_epoch_freq: 5                             
         save_latest_freq: 5000                          
           serial_batches: False                         
                   suffix:                               
         update_html_freq: 1000                          
                use_wandb: True                          	[default: False]
                  verbose: False                         
----------------- End -------------------
dataset [UnalignedDataset] was created
The number of training images = 600
dataset [UnalignedDataset] was created
patience: 10
[UnetSkipConnectionBlock(
  (model): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): UnetSkipConnectionBlock(
      (model): Sequential(
        (0): LeakyReLU(negative_slope=0.2, inplace=True)
        (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (3): UnetSkipConnectionBlock(
          (model): Sequential(
            (0): LeakyReLU(negative_slope=0.2, inplace=True)
            (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            (2): ReLU(inplace=True)
            (3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): ReLU(inplace=True)
        (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (6): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (4): Tanh()
  )
)]
initialize network with normal
[UnetSkipConnectionBlock(
  (model): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (1): UnetSkipConnectionBlock(
      (model): Sequential(
        (0): LeakyReLU(negative_slope=0.2, inplace=True)
        (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
        (3): UnetSkipConnectionBlock(
          (model): Sequential(
            (0): LeakyReLU(negative_slope=0.2, inplace=True)
            (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            (2): ReLU(inplace=True)
            (3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
            (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
          )
        )
        (4): ReLU(inplace=True)
        (5): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        (6): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
      )
    )
    (2): ReLU(inplace=True)
    (3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    (4): Tanh()
  )
)]
initialize network with normal
initialize network with normal
initialize network with normal
model [CycleGANModel] was created
---------- Networks initialized -------------
[Network G_A] Total number of parameters : 0.928 M
[Network G_B] Total number of parameters : 0.928 M
[Network D_A] Total number of parameters : 2.765 M
[Network D_B] Total number of parameters : 2.765 M
-----------------------------------------------
wandb: wandb version 0.13.2 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.12.9
wandb: Syncing run ellen_dwt_uresnet1_1_512n1000_0831
wandb: ⭐️ View project at https://wandb.ai/ellen/CycleGAN-and-pix2pix
wandb: 🚀 View run at https://wandb.ai/ellen/CycleGAN-and-pix2pix/runs/b8vblcqv
wandb: Run data is saved locally in /home/guest1/ellen_code/pytorch-CycleGAN-and-pix2pix_ellen/wandb/run-20220901_165348-b8vblcqv
wandb: Run `wandb offline` to turn off syncing.

create web directory ./checkpoints/ellen_dwt_uresnet1_1_512n1000_0831/web...
learning rate 0.0002000 -> 0.0002000
/home/guest1/.conda/envs/ellen/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
(epoch: 1, iters: 100, time: 0.098, data: 0.226) D_A: 0.206 G_A: 0.540 cycle_A: 13.334 idt_A: 6.303 D_B: 0.231 G_B: 0.567 cycle_B: 6.303 idt_B: 3.333 
(epoch: 1, iters: 200, time: 0.096, data: 0.005) D_A: 0.151 G_A: 0.590 cycle_A: 13.728 idt_A: 6.172 D_B: 0.234 G_B: 0.943 cycle_B: 6.172 idt_B: 3.431 
(epoch: 1, iters: 300, time: 0.143, data: 0.005) D_A: 0.159 G_A: 0.617 cycle_A: 12.893 idt_A: 6.543 D_B: 0.161 G_B: 0.687 cycle_B: 6.543 idt_B: 3.221 
(epoch: 1, iters: 400, time: 0.873, data: 0.005) D_A: 0.156 G_A: 0.779 cycle_A: 13.824 idt_A: 6.264 D_B: 0.134 G_B: 0.879 cycle_B: 6.268 idt_B: 3.450 
(epoch: 1, iters: 500, time: 0.126, data: 0.004) D_A: 0.807 G_A: 1.899 cycle_A: 12.307 idt_A: 6.468 D_B: 0.140 G_B: 0.799 cycle_B: 6.467 idt_B: 3.068 
(epoch: 1, iters: 600, time: 0.102, data: 0.003) D_A: 0.215 G_A: 2.007 cycle_A: 12.022 idt_A: 6.176 D_B: 0.085 G_B: 0.579 cycle_B: 6.177 idt_B: 2.995 
End of epoch 1 / 400 	 Time Taken: 77 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 2, iters: 100, time: 0.144, data: 0.227) D_A: 0.063 G_A: 0.709 cycle_A: 11.066 idt_A: 5.916 D_B: 0.068 G_B: 0.653 cycle_B: 5.911 idt_B: 2.771 
(epoch: 2, iters: 200, time: 0.681, data: 0.004) D_A: 0.058 G_A: 0.797 cycle_A: 11.091 idt_A: 6.211 D_B: 0.049 G_B: 0.795 cycle_B: 6.209 idt_B: 2.811 
(epoch: 2, iters: 300, time: 0.117, data: 0.005) D_A: 0.053 G_A: 0.689 cycle_A: 10.146 idt_A: 6.095 D_B: 2.552 G_B: 4.004 cycle_B: 6.080 idt_B: 2.668 
(epoch: 2, iters: 400, time: 0.101, data: 0.004) D_A: 0.314 G_A: 1.732 cycle_A: 9.922 idt_A: 5.965 D_B: 0.024 G_B: 0.886 cycle_B: 5.934 idt_B: 2.570 
(epoch: 2, iters: 500, time: 0.135, data: 0.004) D_A: 0.081 G_A: 0.995 cycle_A: 8.316 idt_A: 6.180 D_B: 0.023 G_B: 0.913 cycle_B: 6.144 idt_B: 2.141 
(epoch: 2, iters: 600, time: 0.339, data: 0.004) D_A: 0.076 G_A: 0.969 cycle_A: 8.291 idt_A: 5.919 D_B: 0.021 G_B: 0.990 cycle_B: 5.849 idt_B: 2.117 
End of epoch 2 / 400 	 Time Taken: 77 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 3, iters: 100, time: 0.102, data: 0.199) D_A: 0.030 G_A: 1.012 cycle_A: 7.613 idt_A: 5.367 D_B: 0.026 G_B: 1.080 cycle_B: 5.263 idt_B: 1.913 
(epoch: 3, iters: 200, time: 0.138, data: 0.005) D_A: 0.018 G_A: 0.896 cycle_A: 6.814 idt_A: 5.339 D_B: 0.019 G_B: 0.982 cycle_B: 5.237 idt_B: 1.735 
(epoch: 3, iters: 300, time: 0.185, data: 0.004) D_A: 0.020 G_A: 0.862 cycle_A: 6.160 idt_A: 5.464 D_B: 0.011 G_B: 0.915 cycle_B: 5.383 idt_B: 1.557 
(epoch: 3, iters: 400, time: 0.629, data: 0.004) D_A: 0.019 G_A: 0.966 cycle_A: 6.762 idt_A: 4.820 D_B: 0.044 G_B: 0.966 cycle_B: 4.786 idt_B: 1.684 
(epoch: 3, iters: 500, time: 0.100, data: 0.004) D_A: 0.047 G_A: 0.957 cycle_A: 4.898 idt_A: 4.580 D_B: 0.026 G_B: 0.968 cycle_B: 4.590 idt_B: 1.243 
(epoch: 3, iters: 600, time: 0.149, data: 0.004) D_A: 0.020 G_A: 1.031 cycle_A: 6.238 idt_A: 3.903 D_B: 0.010 G_B: 0.970 cycle_B: 3.752 idt_B: 1.604 
End of epoch 3 / 400 	 Time Taken: 75 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 4, iters: 100, time: 0.096, data: 0.212) D_A: 0.016 G_A: 0.933 cycle_A: 5.599 idt_A: 3.610 D_B: 0.013 G_B: 0.951 cycle_B: 3.456 idt_B: 1.386 
(epoch: 4, iters: 200, time: 0.706, data: 0.004) D_A: 0.028 G_A: 0.944 cycle_A: 5.386 idt_A: 3.472 D_B: 0.014 G_B: 0.886 cycle_B: 3.546 idt_B: 1.389 
(epoch: 4, iters: 300, time: 0.091, data: 0.004) D_A: 0.023 G_A: 0.908 cycle_A: 4.860 idt_A: 3.433 D_B: 0.013 G_B: 0.983 cycle_B: 3.324 idt_B: 1.198 
(epoch: 4, iters: 400, time: 0.094, data: 0.004) D_A: 0.024 G_A: 0.964 cycle_A: 4.884 idt_A: 3.097 D_B: 0.013 G_B: 0.987 cycle_B: 2.928 idt_B: 1.241 
(epoch: 4, iters: 500, time: 0.103, data: 0.004) D_A: 0.243 G_A: 0.388 cycle_A: 5.080 idt_A: 3.046 D_B: 0.013 G_B: 1.018 cycle_B: 3.108 idt_B: 1.240 
(epoch: 4, iters: 600, time: 0.326, data: 0.007) D_A: 0.208 G_A: 0.296 cycle_A: 4.438 idt_A: 2.952 D_B: 0.011 G_B: 0.971 cycle_B: 2.650 idt_B: 1.145 
End of epoch 4 / 400 	 Time Taken: 79 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 5, iters: 100, time: 0.100, data: 0.197) D_A: 0.243 G_A: 0.321 cycle_A: 3.985 idt_A: 2.248 D_B: 0.035 G_B: 0.877 cycle_B: 2.249 idt_B: 1.018 
(epoch: 5, iters: 200, time: 0.151, data: 0.004) D_A: 0.217 G_A: 0.469 cycle_A: 3.213 idt_A: 2.876 D_B: 0.013 G_B: 0.964 cycle_B: 2.978 idt_B: 0.800 
(epoch: 5, iters: 300, time: 0.093, data: 0.004) D_A: 0.138 G_A: 0.463 cycle_A: 3.760 idt_A: 2.101 D_B: 0.012 G_B: 0.979 cycle_B: 2.014 idt_B: 0.961 
(epoch: 5, iters: 400, time: 0.666, data: 0.008) D_A: 0.273 G_A: 0.513 cycle_A: 3.687 idt_A: 2.124 D_B: 0.049 G_B: 0.835 cycle_B: 2.189 idt_B: 0.984 
(epoch: 5, iters: 500, time: 0.103, data: 0.004) D_A: 0.117 G_A: 0.774 cycle_A: 2.949 idt_A: 1.881 D_B: 0.019 G_B: 0.865 cycle_B: 1.887 idt_B: 0.754 
(epoch: 5, iters: 600, time: 0.115, data: 0.004) D_A: 0.040 G_A: 0.647 cycle_A: 3.323 idt_A: 1.971 D_B: 0.064 G_B: 1.153 cycle_B: 1.962 idt_B: 0.838 
saving the model at the end of epoch 5, iters 3000
End of epoch 5 / 400 	 Time Taken: 77 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 6, iters: 100, time: 0.100, data: 0.193) D_A: 0.025 G_A: 0.788 cycle_A: 3.331 idt_A: 2.092 D_B: 0.021 G_B: 0.890 cycle_B: 2.118 idt_B: 0.856 
(epoch: 6, iters: 200, time: 0.748, data: 0.003) D_A: 0.104 G_A: 0.690 cycle_A: 2.661 idt_A: 1.629 D_B: 0.093 G_B: 0.903 cycle_B: 1.645 idt_B: 0.689 
(epoch: 6, iters: 300, time: 0.103, data: 0.004) D_A: 0.114 G_A: 0.862 cycle_A: 3.516 idt_A: 1.710 D_B: 0.019 G_B: 0.935 cycle_B: 1.685 idt_B: 0.898 
(epoch: 6, iters: 400, time: 0.101, data: 0.003) D_A: 0.031 G_A: 0.945 cycle_A: 3.175 idt_A: 1.696 D_B: 0.059 G_B: 0.760 cycle_B: 1.744 idt_B: 0.852 
(epoch: 6, iters: 500, time: 0.101, data: 0.003) D_A: 0.060 G_A: 0.723 cycle_A: 2.288 idt_A: 1.489 D_B: 0.025 G_B: 0.790 cycle_B: 1.460 idt_B: 0.599 
(epoch: 6, iters: 600, time: 0.386, data: 0.003) D_A: 0.294 G_A: 0.513 cycle_A: 2.438 idt_A: 1.499 D_B: 0.011 G_B: 0.934 cycle_B: 1.485 idt_B: 0.650 
End of epoch 6 / 400 	 Time Taken: 80 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 7, iters: 100, time: 0.091, data: 0.196) D_A: 0.086 G_A: 0.784 cycle_A: 2.766 idt_A: 1.555 D_B: 0.012 G_B: 0.851 cycle_B: 1.532 idt_B: 0.725 
(epoch: 7, iters: 200, time: 0.112, data: 0.005) D_A: 0.263 G_A: 1.128 cycle_A: 2.523 idt_A: 1.637 D_B: 0.141 G_B: 0.760 cycle_B: 1.620 idt_B: 0.630 
(epoch: 7, iters: 300, time: 0.159, data: 0.004) D_A: 0.049 G_A: 0.635 cycle_A: 2.067 idt_A: 1.639 D_B: 0.011 G_B: 0.860 cycle_B: 1.573 idt_B: 0.516 
(epoch: 7, iters: 400, time: 0.716, data: 0.004) D_A: 0.021 G_A: 0.791 cycle_A: 1.898 idt_A: 1.414 D_B: 0.011 G_B: 1.021 cycle_B: 1.401 idt_B: 0.481 
(epoch: 7, iters: 500, time: 0.138, data: 0.003) D_A: 0.033 G_A: 0.999 cycle_A: 1.792 idt_A: 1.413 D_B: 0.008 G_B: 0.958 cycle_B: 1.369 idt_B: 0.456 
(epoch: 7, iters: 600, time: 0.091, data: 0.005) D_A: 0.037 G_A: 0.730 cycle_A: 1.989 idt_A: 1.274 D_B: 0.018 G_B: 0.990 cycle_B: 1.231 idt_B: 0.491 
End of epoch 7 / 400 	 Time Taken: 77 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 8, iters: 100, time: 0.104, data: 0.177) D_A: 0.071 G_A: 0.713 cycle_A: 2.269 idt_A: 1.222 D_B: 0.016 G_B: 0.782 cycle_B: 1.233 idt_B: 0.600 
(epoch: 8, iters: 200, time: 0.809, data: 0.004) D_A: 0.034 G_A: 0.850 cycle_A: 2.331 idt_A: 1.498 D_B: 0.236 G_B: 0.201 cycle_B: 1.317 idt_B: 0.573 
(epoch: 8, iters: 300, time: 0.095, data: 0.005) D_A: 0.038 G_A: 1.005 cycle_A: 1.852 idt_A: 1.215 D_B: 0.201 G_B: 0.319 cycle_B: 1.197 idt_B: 0.469 
(epoch: 8, iters: 400, time: 0.103, data: 0.005) D_A: 0.192 G_A: 1.169 cycle_A: 2.164 idt_A: 1.118 D_B: 0.167 G_B: 0.391 cycle_B: 1.056 idt_B: 0.589 
(epoch: 8, iters: 500, time: 0.095, data: 0.004) D_A: 0.034 G_A: 0.910 cycle_A: 2.069 idt_A: 1.340 D_B: 0.120 G_B: 0.513 cycle_B: 1.314 idt_B: 0.470 
(epoch: 8, iters: 600, time: 0.426, data: 0.003) D_A: 0.417 G_A: 0.186 cycle_A: 1.906 idt_A: 1.070 D_B: 0.229 G_B: 0.818 cycle_B: 1.047 idt_B: 0.475 
End of epoch 8 / 400 	 Time Taken: 81 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 9, iters: 100, time: 0.090, data: 0.197) D_A: 0.065 G_A: 0.918 cycle_A: 2.565 idt_A: 1.228 D_B: 0.248 G_B: 0.418 cycle_B: 1.127 idt_B: 0.658 
(epoch: 9, iters: 200, time: 0.100, data: 0.003) D_A: 0.038 G_A: 0.878 cycle_A: 1.940 idt_A: 1.049 D_B: 0.163 G_B: 0.655 cycle_B: 1.094 idt_B: 0.592 
(epoch: 9, iters: 300, time: 0.145, data: 0.004) D_A: 0.184 G_A: 1.298 cycle_A: 2.083 idt_A: 0.924 D_B: 0.082 G_B: 0.713 cycle_B: 1.100 idt_B: 0.505 
(epoch: 9, iters: 400, time: 0.836, data: 0.004) D_A: 0.069 G_A: 0.795 cycle_A: 1.918 idt_A: 0.928 D_B: 0.754 G_B: 1.338 cycle_B: 0.940 idt_B: 0.478 
(epoch: 9, iters: 500, time: 0.148, data: 0.003) D_A: 0.256 G_A: 0.941 cycle_A: 2.138 idt_A: 1.302 D_B: 0.099 G_B: 0.740 cycle_B: 1.225 idt_B: 0.584 
(epoch: 9, iters: 600, time: 0.134, data: 0.005) D_A: 0.052 G_A: 0.665 cycle_A: 1.709 idt_A: 0.946 D_B: 0.051 G_B: 0.760 cycle_B: 1.106 idt_B: 0.440 
End of epoch 9 / 400 	 Time Taken: 78 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 10, iters: 100, time: 0.180, data: 0.185) D_A: 0.079 G_A: 0.467 cycle_A: 1.669 idt_A: 0.979 D_B: 0.049 G_B: 0.435 cycle_B: 0.989 idt_B: 0.405 
(epoch: 10, iters: 200, time: 0.905, data: 0.005) D_A: 0.239 G_A: 0.747 cycle_A: 1.397 idt_A: 0.875 D_B: 0.072 G_B: 0.713 cycle_B: 0.940 idt_B: 0.323 
(epoch: 10, iters: 300, time: 0.108, data: 0.004) D_A: 0.032 G_A: 0.663 cycle_A: 1.691 idt_A: 0.897 D_B: 0.032 G_B: 0.615 cycle_B: 0.993 idt_B: 0.368 
(epoch: 10, iters: 400, time: 0.102, data: 0.004) D_A: 0.016 G_A: 0.790 cycle_A: 2.228 idt_A: 0.950 D_B: 0.043 G_B: 0.915 cycle_B: 1.081 idt_B: 0.713 
(epoch: 10, iters: 500, time: 0.102, data: 0.005) D_A: 0.088 G_A: 0.475 cycle_A: 1.759 idt_A: 0.905 D_B: 0.036 G_B: 0.968 cycle_B: 0.866 idt_B: 0.395 
(epoch: 10, iters: 600, time: 0.712, data: 0.004) D_A: 0.082 G_A: 0.410 cycle_A: 1.716 idt_A: 1.021 D_B: 0.027 G_B: 0.700 cycle_B: 1.029 idt_B: 0.578 
saving the model at the end of epoch 10, iters 6000
End of epoch 10 / 400 	 Time Taken: 81 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 11, iters: 100, time: 0.102, data: 0.191) D_A: 0.061 G_A: 1.071 cycle_A: 1.551 idt_A: 0.823 D_B: 0.048 G_B: 0.739 cycle_B: 0.871 idt_B: 0.354 
(epoch: 11, iters: 200, time: 0.103, data: 0.004) D_A: 0.022 G_A: 0.724 cycle_A: 2.028 idt_A: 0.932 D_B: 0.059 G_B: 1.184 cycle_B: 0.998 idt_B: 0.445 
(epoch: 11, iters: 300, time: 0.103, data: 0.004) D_A: 0.066 G_A: 1.167 cycle_A: 1.615 idt_A: 0.759 D_B: 0.018 G_B: 1.011 cycle_B: 0.827 idt_B: 0.409 
(epoch: 11, iters: 400, time: 0.794, data: 0.004) D_A: 0.103 G_A: 0.607 cycle_A: 2.158 idt_A: 0.900 D_B: 0.187 G_B: 0.953 cycle_B: 0.906 idt_B: 0.548 
(epoch: 11, iters: 500, time: 0.090, data: 0.004) D_A: 0.124 G_A: 0.227 cycle_A: 1.594 idt_A: 0.864 D_B: 0.020 G_B: 0.764 cycle_B: 0.896 idt_B: 0.350 
(epoch: 11, iters: 600, time: 0.166, data: 0.005) D_A: 0.027 G_A: 1.009 cycle_A: 2.065 idt_A: 0.782 D_B: 0.029 G_B: 0.706 cycle_B: 0.992 idt_B: 0.529 
End of epoch 11 / 400 	 Time Taken: 79 sec
learning rate 0.0002000 -> 0.0002000
(epoch: 12, iters: 100, time: 0.147, data: 0.177) D_A: 0.117 G_A: 1.100 cycle_A: 1.389 idt_A: 0.927 D_B: 0.069 G_B: 0.901 cycle_B: 0.885 idt_B: 0.466 
(epoch: 12, iters: 200, time: 0.872, data: 0.006) D_A: 0.062 G_A: 0.615 cycle_A: 1.412 idt_A: 0.757 D_B: 0.021 G_B: 0.653 cycle_B: 0.886 idt_B: 0.309 
(epoch: 12, iters: 300, time: 0.125, data: 0.004) D_A: 0.043 G_A: 0.607 cycle_A: 1.916 idt_A: 0.775 D_B: 0.034 G_B: 1.116 cycle_B: 0.883 idt_B: 0.444 
(epoch: 12, iters: 400, time: 0.102, data: 0.004) D_A: 0.253 G_A: 0.236 cycle_A: 1.384 idt_A: 0.805 D_B: 0.018 G_B: 0.933 cycle_B: 0.791 idt_B: 0.332 
(epoch: 12, iters: 500, time: 0.135, data: 0.006) D_A: 0.059 G_A: 0.713 cycle_A: 1.444 idt_A: 0.682 D_B: 0.019 G_B: 0.916 cycle_B: 0.808 idt_B: 0.357 
(epoch: 12, iters: 600, time: 0.512, data: 0.004) D_A: 0.119 G_A: 0.183 cycle_A: 1.388 idt_A: 0.826 D_B: 0.018 G_B: 0.865 cycle_B: 0.917 idt_B: 0.364 
End of epoch 12 / 400 	 Time Taken: 80 sec
learning rate 0.0002000 -> 0.0002000
